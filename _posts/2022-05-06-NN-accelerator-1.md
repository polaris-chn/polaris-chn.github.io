---
layout: post
title: 神经网络加速器概述
categories: NN Accelerator
description: 对于神经网络加速器的简述
keywords: NN Accelerator
---

# 神经网络加速器概述

## 背景
近年来，人工智能得到了越来越广泛的应用，而神经网络算法作为一种成熟高效的人工智能算法被应用在各种业务场景。由于神经网络低复杂度高数据量的特点，传统的CPU或者其他专用芯片很难高效执行神经网络算法，所以开发专用的神经网络加速器成为一种必要。

人工智能芯片的发展势不可挡，其发展趋势正从云端逐渐下沉到边缘设备和物联网节点，而下沉会带来市场规模的急剧扩大。所以人工智能在云端和边缘端，都需要有能执行人工智能计算的硬件载体。

**<font color=red>分析学界和产业界对于AI芯片和加速器的不同思考角度</font>**

学界在硬件方面似乎专注于新技术，例如**存内计算，图神经网络硬件化，类脑计算硬件化**等；业界在硬件方面比较保守，技术上的设计原则还是**存储层次化，数据复用，片上互联**等技术，在软件上的注意力放在**编译器和量化工具**上，还是努力在端侧落实AI算法。

虽然困难重重，但是AI是大势所趋，AI芯片也是必然需求，对相关从业者来说，需要考量的就是，未来AI芯片会走向何方。


## AI芯片
AI芯片一般泛指所有用来加速AI应用，尤其是用在基于神经网络的深度学习中的硬件，根据其技术架构来分，可分为GPU, FPGA, ASIC及类脑芯片，同时CPU可执行通用AI计算，类脑芯片还处于探索阶段。AI芯片根据其在网络中的位置可分为云端AI芯片，边缘及终端AI芯片；根据其在实践中的目标，可分为训练（train）芯片和推理（inference）芯片。云端芯片主要承担训练和推理任务，具体指智能数据分析、模型训练任务和部分对传输带宽要求比较高的推理任务；边缘和终端芯片主要承担推理任务，需要独立完成数据集收集、环境感知、人机交互及部分推理决策控制任务。

类脑神经结构芯片以IBM TrueNorth为代表

## 有用链接
[深度学习编译、芯片](https://zhuanlan.zhihu.com/p/269070510)

[神经网络专用架构](https://zhuanlan.zhihu.com/p/43554156)

[AI芯片架构CPU,GPU,FPGA,类脑](https://zhuanlan.zhihu.com/p/467076505)

